---
layout:     post
title:      AIAgent之成本控制
subtitle:   成本管理
date:       2026-02-28
author:     LuochuanAD
header-img: img/home_blog_background.jpg
catalog: true
tags:
- AIAgent

---

## 背景

> 在做AI产品的过程中,随着用户数的增加,每次调用LLM, 使用的提示词都在花钱. 那么如何在不影响结果的前提下,极限的减少不必要的消费呢?

## LLM的收费结构

> 这里举例OpenAI

![](https://raw.githubusercontent.com/LuochuanAD/BlogSourceImage/refs/heads/master/BlogSourceImage/BlogSourceImage2026/openAI_prices.png)

可以看出**最新的openAI API的Tokens收费结构为:**

```
Input 输入: 是Cached input的 10倍价格, 是Output的 1/10的价格

Cached input 缓存输入: 最便宜, 是Input的 1/10的价格, Output的 1/100的价格

Output 输出: 最贵, 是Input的10倍价格, 是Cached input的100倍价格

```
对于旧版的LLM: 如gpt-4o系列 则取消了Cached input 缓存输入的tokens费用. 


## 推荐方案

### 1, 提示用户输入要简短,确定性高的关键词

```
	注意这里用的是“提示用户”,而不是“限制用户”.  
	对于一个完整的的企业AIAgent来说,不能限制用户输入消息的长度, 因为用户输入的消息都是要完整保持原本意义的发送到LLM的.

	提示用户输入要简短的语句 =》 可以有效减少上下文的长度Tokens

	提示用户输入要确定性高的语句而不是模糊性语句 =》 可以提升模型生成的准确性,减少幻觉输出

	提示用户输入要输入关键词语句 =》 可以提升检索的准确性,同时也可以减少上下文的长度Tokens

```
### 2, 限制LLM输出长度

```
LLM输出的内容也是最贵的, 可以在prompt提示词中,

明确提示输出固定长度的内容,或者使用总结性的,概括性的语句来输出 

=》可以有效降低Output输出长度的Tokens

```

### 3, 减少LLM的调用次数: memory的设计很有必要

```
虽然每次调用LLM, openAI会自动计算Cached input,降低成本,但是不确定很大,所以对话的历史记录就很有必要保存下来了.

关于memory这里涉及到长期和短期记忆,本文不会介绍,仅仅讲解如何减少token的使用而设计memory.

用户短期内重复性的提问,不需要每次调用LLM, 可以在对话记录中搜索结果 =》 减少调用LLM的次数

如何充分利用Cached input 缓存输入设计memory,来减少tokens费用?

Cached input 缓存输入 一个最重要的原则: 能不动的尽量别动,最好是一个词都不要改变.

1, memory 的对话记录要进行分类,同类问题的记录放在一起 =》有助于检索准确性和减少LLM的调用

2, memory的对话记录要进行总结 =》方便后续对话时作为输入的一部分,减少上下文长度


```

### 4, 使用prompt提示词压缩算法

```
使用结构化的提示词,一般会非常长, 而提示词的长度也会影响tokens的收费, 

这里给出一个方案:提示词压缩算法LLMLingua-2. 

这里不做过多解释,有兴趣可以详细了解一下

```

### 5, prompt提示词写法: 充分利用Cached input 缓存输入

```
在每一次调用LLM时,prompt提示词都会使用,

prompt里面的提示词会根据用户的输入,RAG的检索结果等等,进行调整或补充.

要想充分Cached input 缓存输入规则,降低成本,则是要改变提示词里面的顺序, 这样不变的内容会被缓存,降低费用.

```
**举例:**

```
prompt = “

	你是一个商业案例分析时,擅长根据公司的财务报表来分析风险.
	已经知道以下背景信息:
	{Background_msg}

	和财务报表内容:
	{Financial_msg}
	
	根据用户问题:
	{question}

	回答结果.
	返回结果的格式:
	return:
	{
		result:""
	}

”
第一眼看上去此prompt提示词没问题, 但根据Cached input 缓存输入的规则,只有前两行内容会进入缓存.

下面是利用Cached input 缓存输入的规则,改变后的prompt:

cached_prompt = “

	你是一个商业案例分析时,擅长根据公司的财务报表来分析风险.
	基于 背景信息和 财务报表内容,根据 用户问题 回答结果.
	返回结果的格式:
	return:
	{
		result:""
	}

	背景信息: {Background_msg}
	财务报表内容: {Financial_msg}
	用户问题: {question}
”
根据Cached input 缓存输入的规则,改进后的cached_prompt会缓存前七行内容,费用将为 input的1/10价格

```

### 6, 是否必须要调用LLM

**举例:**
```
将半结构化或者无结构的文档存入到向量数据库时. 为了提高准确性是否需要调用LLM将原始文档数据转为结构化的数据格式:如json. 

这里提出一个方案: 

1, 正则表达式过滤,

2, 基于规则解析,

3, XGBoost分类算法,

4, 关键词查找,

来将半结构化或者无结构的文档转为结构化的数据.

```
请参考下面这篇文章:“RAG之Structural Chunks设计”

[https://strictfrog.com/2026/01/31/RAG%E4%B9%8BStructural-Chunks%E8%AE%BE%E8%AE%A1%E4%B8%8E%E6%80%9D%E8%80%83/](https://strictfrog.com/2026/01/31/RAG%E4%B9%8BStructural-Chunks%E8%AE%BE%E8%AE%A1%E4%B8%8E%E6%80%9D%E8%80%83/)

### 7, 在不影响结果质量的前提下,是否可以切换LLM

```
建议切换GPT-5同系列下的模型, 但要进行大量测试生成结果.

一般来说最新的模型肯定是最好的,但不同的模型擅长的功能可能有区别,这需要AI工程师进行验证和判断了.

```


### 8, 是否必须要调用LLM的API

```
例如: 调用whisper模型的API将语音转为文字是需要花钱. 

那么本地部署whisper代码是免费的. 

使用TTS也是免费,可以有很多的平替方案的

```
### 9, 用户输入长度限制: Max_tokens

```
结合特定的AIAgent, 判断:

1, 是否要设置Max_tokens,

2,设置最大的Max_tokens为多少. 

个人认为:

如果是企业级AIAgent,只提供给企业内部人员使用,不设置为好, 因为企业内部的人不会故意去大量消耗tokens.

如果用户是提供给外部人员使用,那肯定需要设置Max_token,并且对于用户的使用tokens进行监控和限制.

```

## 总结

**一次一分钱很少, 1万人使用10次,那就很多了.**













